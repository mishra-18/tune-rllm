The preference dataset used for fine tuning; Intel/orca_dpo_pairs. Used for finetuning the ```Phi-3-mini-4k-instruct``` model with the minimal GPU resource under the free-tier Colab environment.

Direct Preference Optimization (DPO) follows the same reward based methods with little changes, where the goal is to align the base models outputs with human preferences while using the reference model as a benchmark.

You can access the [Wandb Logs](https://wandb.ai/mishra4-deeplogic-ai/huggingface/reports/train-loss-24-07-22-22-08-06---Vmlldzo4NzY2NzU4?accessToken=2jdm6mkp6fuqn7z0nkf7d90swgrsk80ritosa4obig60c23dnrcco19dj1b9et53).

Checkout Phi3-TheFinetunedOne on [HuggingFace](https://huggingface.co/smishr-18/Phi3-TheFinetunedOne).

<p align="center">
  <img src="https://github.com/user-attachments/assets/e32989d5-4b25-4acc-8191-b4ee041a9d93" alt="Image Description" width="600"/>
</p>
